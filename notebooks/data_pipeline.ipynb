{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2541b70f-7085-42b2-803c-2e90da2c5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile, BadZipFile\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__ (self, path_rd, extract_dir, img_size=(200,200), batch_size=32):\n",
    "        \"\"\" \n",
    "        Initialize the data pipeline.\n",
    "        \n",
    "        Args:\n",
    "            path_rd (str): Path to the compressed (ZIP) dataset file containing the raw-data.\n",
    "            extract_dir (str): Destination directory for extracted datase.\n",
    "            img_size (tuple): Target image dimensions (height, width).\n",
    "            batch_size (int): Size of training batches.\n",
    "        \"\"\"\n",
    "        self.path_rd = path_rd\n",
    "        self.extract_dir = extract_dir\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.class_names = [\"crazing\",\"inclusion\",\"patches\",\"pitted-surface\", \"rolled-in-scale\", \"scratches\"]\n",
    "\n",
    "    def running_engine(self)\n",
    "        \"\"\"Execute the full pipeline and return prepared datasets.\"\"\"\n",
    "        self._extract_data()\n",
    "        self.organize_flat_structure()\n",
    "        return self._create_datasets()\n",
    "\n",
    "    def extract_data(self)\n",
    "        \"\"\"Step 1: Data Ingestion & Data Extraction from ZIP archive.\"\"\"\n",
    "            if not os.path.exists(self.extract_to) or len(os.listdir(self.extract_to)) == 0:\n",
    "                print(f\" Extracting data from {self.zip_path}...\")\n",
    "                with ZipFile (self.path_rd, \"r\") as zip:\n",
    "                    zip.extractall(self.extract_to)\n",
    "                print (\"Extraction completed successfully\")\n",
    "            else: \n",
    "                print(\"Data already exists on the disk\")\n",
    "\n",
    "    def _organize_flat_structure(self):\n",
    "        \"\"\"\n",
    "        Step 2: Organize flat images into class-named subfolders.\n",
    "        \"\"\"\n",
    "        print(\" Organizing images into class-specific folders...\")\n",
    "        for split in [\"train\", \"valid\", \"test\"]\n",
    "            split_path = os.path.join(self.extract_to, split)\n",
    "\n",
    "            if not os.path.exists(split_path):\n",
    "                continue\n",
    "\n",
    "            for filename in os.listdir(split_path):\n",
    "                file_path = os.path.join(split_path, filname):\n",
    "                    \n",
    "                # --- Skip already organized directories --- \n",
    "                if not os.path.exists(file_path):\n",
    "                    continue \n",
    "\n",
    "                for class_name in self.class_names:\n",
    "                    if filename.lower().startswith(class_name.lower()):\n",
    "                        target_dir = os.path.join(split_path, class_name)\n",
    "                        os.makedirs(target_dir, exist_ok=True)\n",
    "                        shutil.move(file_path, os.path.join(target_dir, filename))\n",
    "                    break\n",
    "\n",
    "        \n",
    "        \n",
    "    def create_datasets (self):\n",
    "        \"\"\"\n",
    "        Step 3: Loading - Creating tf.data.Dataset objects from organized directories.\n",
    "        \"\"\"\n",
    "\n",
    "        # --- Paths to pre-defined splits --- \n",
    "        train_path = os.path.join(self.extract_to, \"train\")\n",
    "        valid_path = os.path.join(self.extract_to, \"valid\")\n",
    "        test_path = os.path.join(self.extract_to, \"test\")\n",
    "\n",
    "        # Load datasets without 'validation_split' to keep the original distribution\n",
    "        train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "            train_path, image_size = self.imag_size, batch_size = self.batch_size, label_mode = \"categorical\"\n",
    "        )\n",
    "\n",
    "        valid_data = tf.keras.utils.image_dataset_from_directory (\n",
    "            valid_path, image_size = self.image_size, batch_size = self.batch_size, label_mode = \"categorical\"\n",
    "        )\n",
    "\n",
    "        test_data = tf.keras.utils.image_dataset_from_directory (\n",
    "            test_path, image_size = self.image_size, batch_size = self.batch_size, lbale_mode = \"categorical\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "           \n",
    "            \n",
    "\n",
    "        def \n",
    "                            \n",
    "                    \n",
    "                    \n",
    "    \n",
    "\n",
    "                    \n",
    "               \n",
    "\n",
    "        train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "            self.extract_to,\n",
    "            validation_split = 0.2,\n",
    "            \n",
    "            \n",
    "        )\n",
    "                \n",
    "                \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
